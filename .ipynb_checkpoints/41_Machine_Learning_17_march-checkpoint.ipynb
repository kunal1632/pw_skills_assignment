{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eee51403",
   "metadata": {},
   "source": [
    "### Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4726d0ac",
   "metadata": {},
   "source": [
    "Missing values refer to the absence of a particular value or data point in a dataset. In practical scenarios, missing values can occur due to various reasons, such as data collection errors, incomplete surveys, or information not being available for certain observations. \n",
    "\n",
    "Handling missing values is essential for several reasons:\n",
    "\n",
    "1. Avoiding biased or incorrect results: If missing values are not appropriately handled, it can lead to biased or incorrect results during data analysis and modeling. It can distort statistical measures, relationships, and predictions.\n",
    "\n",
    "2. Preserving the integrity of the dataset: Missing values can disrupt the integrity of the dataset, making it difficult to draw meaningful insights and conclusions. It is essential to handle missing values to maintain the quality and reliability of the data.\n",
    "\n",
    "3. Utilizing all available information: Missing values may contain valuable information, and ignoring them can lead to a loss of important insights. By handling missing values appropriately, you can make the most of the available data and extract more accurate and comprehensive knowledge.\n",
    "\n",
    "Some machine learning algorithms are not affected by missing values or can handle them inherently. These algorithms include:\n",
    "\n",
    "1. Tree-based algorithms: Decision trees and ensemble methods like Random Forest and Gradient Boosting are not directly affected by missing values. They can handle missing values by finding alternative splits or imputing missing values during the tree construction process.\n",
    "\n",
    "2. Naive Bayes: Naive Bayes is a probabilistic algorithm that can handle missing values. It calculates probabilities based on available data and makes predictions accordingly. Missing values are typically ignored during the probability calculations.\n",
    "\n",
    "3. K-Nearest Neighbors (KNN): KNN is a non-parametric algorithm that uses the distance between data points to make predictions. It can handle missing values by either ignoring the missing values during distance calculations or imputing them based on the neighboring data points.\n",
    "\n",
    "4. Support Vector Machines (SVM): SVM is a powerful algorithm for classification and regression tasks. It can handle missing values by either ignoring them or imputing them before training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03738f8c",
   "metadata": {},
   "source": [
    "### Q2: List down techniques used to handle missing data. Give an example of each with python code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97f34b4",
   "metadata": {},
   "source": [
    "Removal of Rows or Columns:\n",
    "\n",
    "If the missing data is limited to a few rows or columns, they can be removed from the dataset. However, this approach should be used cautiously as it may result in loss of valuable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a42d7d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4],\n",
    "                   'B': [5, None, 7, 8]})\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_without_missing_rows = df.dropna()\n",
    "\n",
    "# Remove columns with missing values\n",
    "df_without_missing_columns = df.dropna(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5317f1b",
   "metadata": {},
   "source": [
    "Mean, Median, or Mode Imputation:\n",
    "\n",
    "Missing values can be replaced with the mean, median, or mode of the respective feature. This method assumes that the missing values are similar to the existing values in that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6f24e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, None, 4],\n",
    "                   'B': [5, None, 7, None]})\n",
    "\n",
    "# Impute missing values with mean\n",
    "df_imputed_mean = df.fillna(df.mean())\n",
    "\n",
    "# Impute missing values with median\n",
    "df_imputed_median = df.fillna(df.median())\n",
    "\n",
    "# Impute missing values with mode\n",
    "df_imputed_mode = df.fillna(df.mode().iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c30387",
   "metadata": {},
   "source": [
    "Forward or Backward Fill:\n",
    "\n",
    "Missing values can be filled with the previous (forward fill) or next (backward fill) observed value in the dataset. This approach is suitable when missing values follow a pattern or are temporally related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09dbbff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, None, 3, None, 5],\n",
    "                   'B': [None, 2, None, 4, None]})\n",
    "\n",
    "# Forward fill missing values\n",
    "df_forward_filled = df.ffill()\n",
    "\n",
    "# Backward fill missing values\n",
    "df_backward_filled = df.bfill()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97787e2",
   "metadata": {},
   "source": [
    "Using Advanced Imputation Methods:\n",
    "\n",
    "Advanced imputation methods, such as K-Nearest Neighbors (KNN) imputation or Expectation-Maximization (EM) imputation, estimate missing values based on the relationships between features or the underlying distribution of the data.\n",
    "Example (KNN Imputation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6903c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "df = pd.DataFrame({'A': [1, None, 3, None, 5],\n",
    "                   'B': [None, 2, None, 4, None]})\n",
    "\n",
    "# KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "df_imputed_knn = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9385b259",
   "metadata": {},
   "source": [
    "### Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e1d2be",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation in machine learning where the classes or categories in the target variable are not represented equally. In other words, one class has a significantly larger number of instances than the other class(es). For example, in a binary classification problem, if 95% of the instances belong to Class A and only 5% belong to Class B, it represents imbalanced data.\n",
    "\n",
    "If imbalanced data is not handled properly, it can lead to several issues:\n",
    "\n",
    "1. Biased Model Performance: Machine learning models trained on imbalanced data tend to be biased towards the majority class, which can lead to inaccurate predictions. The models may have high accuracy in terms of overall accuracy but perform poorly in correctly predicting the minority class. This is because the model tends to predict the majority class more frequently, assuming that it is the dominant class in the dataset.\n",
    "\n",
    "2. Misleading Evaluation Metrics: Evaluation metrics such as accuracy can be misleading when dealing with imbalanced data. For example, if a model predicts the majority class for all instances, it can still achieve a high accuracy rate due to the imbalance. However, this does not reflect the model's actual ability to correctly classify the minority class, which is often of greater interest.\n",
    "\n",
    "3. Decreased Sensitivity: Imbalanced data can lead to models that have low sensitivity (true positive rate) for the minority class. This means the model may struggle to correctly identify instances of the minority class, which can be crucial in many real-world applications such as fraud detection or medical diagnosis.\n",
    "\n",
    "4. Increased False Positives/Negatives: Models trained on imbalanced data are prone to producing a high number of false positives or false negatives, depending on the class of interest. This can have serious consequences in domains where misclassifications can lead to significant costs or risks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fe951",
   "metadata": {},
   "source": [
    "### Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down- sampling are required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801fc267",
   "metadata": {},
   "source": [
    "#### Up-sampling:\n",
    "Up-sampling involves increasing the number of instances in the minority class to match the number of instances in the majority class. This is typically done by replicating or generating synthetic samples of the minority class.\n",
    "\n",
    "Example:\n",
    "Consider a credit card fraud detection dataset where the majority class represents non-fraudulent transactions (Class A) and the minority class represents fraudulent transactions (Class B). If the dataset has a severe class imbalance, such as 99% Class A instances and only 1% Class B instances, up-sampling can be used. In this case, up-sampling would involve creating additional instances of Class B by replicating existing instances or using techniques like Synthetic Minority Over-sampling Technique (SMOTE) to generate synthetic samples. By up-sampling the minority class, the class distribution becomes more balanced, which helps the model learn and make accurate predictions for the minority class.\n",
    "\n",
    "#### Down-sampling:\n",
    "Down-sampling involves reducing the number of instances in the majority class to match the number of instances in the minority class. This is typically done by randomly selecting a subset of instances from the majority class.\n",
    "\n",
    "Example:\n",
    "Consider a medical diagnosis dataset where the majority class represents healthy patients (Class A) and the minority class represents patients with a specific disease (Class B). If the dataset has a significant class imbalance, such as 90% Class A instances and only 10% Class B instances, down-sampling can be used. In this case, down-sampling would involve randomly selecting a subset of instances from Class A to match the number of instances in Class B. By down-sampling the majority class, the class distribution becomes more balanced, which allows the model to avoid biased predictions towards the majority class and provide better classification for the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e658c54",
   "metadata": {},
   "source": [
    "### Q5: What is data Augmentation? Explain SMOTE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f00e4",
   "metadata": {},
   "source": [
    "\n",
    "Data augmentation is a technique used in machine learning to artificially increase the size of a dataset by creating new samples from existing ones. It is commonly employed when the available data is limited or imbalanced, and more diverse or balanced training examples are desired.\n",
    "\n",
    "Data augmentation involves applying various transformations or modifications to the existing data to create new samples while preserving the class labels. These transformations can include rotation, translation, scaling, flipping, cropping, noise injection, or other domain-specific modifications. By generating augmented samples, the model is exposed to a wider range of variations and becomes more robust to different scenarios during training.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a specific data augmentation technique designed to address the class imbalance problem in machine learning. It focuses on augmenting the minority class to balance the class distribution while avoiding overfitting.\n",
    "\n",
    "Here's how SMOTE works:\n",
    "\n",
    "For each sample in the minority class, SMOTE selects k nearest neighbors from the same class based on a distance metric (typically Euclidean distance).\n",
    "\n",
    "Synthetic samples are then generated by interpolating features between the selected minority sample and its k nearest neighbors. This is done by selecting random proportions along the feature vectors and creating new samples at these interpolated positions.\n",
    "\n",
    "The process is repeated until the desired balance between the minority and majority classes is achieved.\n",
    "\n",
    "SMOTE effectively increases the number of minority class instances by generating synthetic samples that are representative of the minority class distribution. This helps to alleviate the class imbalance issue and allows the model to learn from a more balanced dataset during training. It helps prevent the model from being biased towards the majority class and improves its ability to classify the minority class accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7f9085",
   "metadata": {},
   "source": [
    "### Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ba355d",
   "metadata": {},
   "source": [
    "Outliers are data points that significantly deviate from the general pattern or distribution of the dataset. These are observations that are markedly different from the majority of the data points. Outliers can occur due to various reasons such as measurement errors, data entry errors, or rare events.\n",
    "\n",
    "It is essential to handle outliers for the following reasons:\n",
    "\n",
    "1. Impact on Statistical Measures: Outliers can significantly affect statistical measures such as the mean and standard deviation. These measures are sensitive to extreme values, causing them to be biased or skewed. Consequently, any analysis or modeling based on these measures may yield inaccurate results.\n",
    "\n",
    "2. Distortion of Data Distribution: Outliers can distort the overall distribution of the data. They can artificially inflate the range or spread of the data, making it challenging to understand the true distribution and underlying patterns.\n",
    "\n",
    "3. Influence on Model Performance: Outliers can have a disproportionate impact on model fitting. They can pull the regression line or decision boundary towards themselves, leading to suboptimal models that fail to generalize well to new data. Outliers can also affect the estimation of model parameters, leading to biased and unreliable results.\n",
    "\n",
    "4. Adverse Impact on Assumptions: Outliers can violate assumptions of certain statistical or machine learning algorithms. For example, linear regression assumes that the data follows a normal distribution and that the errors are normally distributed. Outliers can violate these assumptions, compromising the validity and reliability of the model.\n",
    "\n",
    "5. Misinterpretation of Insights: Outliers can mislead data analysis and interpretation. They may represent rare or unique events that do not reflect the general behavior of the data. By not handling outliers, these exceptional cases can result in misleading insights and erroneous conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3026bcf",
   "metadata": {},
   "source": [
    "### Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f68b98",
   "metadata": {},
   "source": [
    "When handling missing data in customer data analysis, several techniques can be employed depending on the nature of the missing data and the specific requirements of the project. Here are some commonly used techniques:\n",
    "\n",
    "1. Deletion:\n",
    "\n",
    "- Listwise deletion: Remove entire rows (observations) that contain missing values. This technique is suitable when the missing data is minimal and randomly distributed.\n",
    "- Pairwise deletion: Retain observations with complete information for specific analyses. This technique allows you to use available data without discarding entire cases, but it can lead to different sample sizes for different analyses.\n",
    "2. Mean, Median, or Mode Imputation:\n",
    "\n",
    "- Replace missing values with the mean, median, or mode of the respective feature. This technique assumes that the missing values are similar to the existing values in that feature. It is commonly used for numeric or categorical variables.\n",
    "3. Forward or Backward Fill:\n",
    "\n",
    "- Fill missing values with the previous (forward fill) or next (backward fill) observed value in the dataset. This approach is suitable when missing values follow a pattern or are temporally related.\n",
    "4. Interpolation:\n",
    "\n",
    "- Use interpolation techniques such as linear interpolation, polynomial interpolation, or spline interpolation to estimate missing values based on the relationship between the available data points.\n",
    "5. Hot-Deck Imputation:\n",
    "\n",
    "- Replace missing values with randomly selected values from similar observations or matching criteria. This technique assumes that similar individuals will have similar values for the missing variable.\n",
    "6. Multiple Imputation:\n",
    "\n",
    "- Generate multiple imputed datasets by creating plausible values for missing data based on observed patterns and relationships in the dataset. Multiple imputation allows for the incorporation of uncertainty associated with missing data.\n",
    "8. Machine Learning-Based Imputation:\n",
    "\n",
    "- Utilize machine learning algorithms to predict missing values based on the available features. Methods like K-Nearest Neighbors (KNN) imputation, Decision Tree-based imputation, or regression models can be used for this purpose.\n",
    "9. Expert Knowledge:\n",
    "\n",
    "- Seek input from domain experts who can provide insights or imputation guidelines based on their subject matter expertise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f728301e",
   "metadata": {},
   "source": [
    "### Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ebf1db",
   "metadata": {},
   "source": [
    "1. Visual Analysis:\n",
    "Visualize the missing data pattern by creating plots such as missing data heatmaps, bar charts, or line plots. Analyze whether the missingness is spread randomly across the dataset or if it exhibits any systematic patterns or dependencies.\n",
    "\n",
    "2. Missingness Summary:\n",
    "Calculate summary statistics related to missingness, such as the percentage of missing values per variable or the percentage of missingness within specific groups or categories. Compare these statistics across variables or groups to identify any patterns or dependencies.\n",
    "\n",
    "3. Missing Data Mechanism:\n",
    "Consider the mechanism that caused the missing data. Missing data can be classified into different mechanisms, such as Missing Completely at Random (MCAR), Missing at Random (MAR), or Missing Not at Random (MNAR). Assess the relationship between the missingness and other variables to understand if there are any patterns or dependencies.\n",
    "\n",
    "4. Missing Data Imputation:\n",
    "Perform missing data imputation using various techniques, such as mean or median imputation, and observe if the imputed values affect the relationship or distribution of other variables. The impact of imputation on the analysis can provide insights into the missing data mechanism.\n",
    "\n",
    "5. Statistical Tests:\n",
    "Conduct statistical tests or hypothesis tests to examine the relationship between missingness and other variables. For example, perform chi-square tests or t-tests to assess if missingness depends on categorical or continuous variables, respectively.\n",
    "\n",
    "6. Model-Based Approaches:\n",
    "Utilize advanced modeling techniques, such as logistic regression or decision trees, to predict missingness based on other variables in the dataset. By assessing the model's performance and examining the importance of predictor variables, you can identify patterns or dependencies in the missing data.\n",
    "\n",
    "7. Domain Expertise:\n",
    "Seek input from domain experts who can provide insights into potential patterns or mechanisms that might contribute to the missing data. Experts can offer valuable knowledge and context specific to the dataset and the field of study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac701728",
   "metadata": {},
   "source": [
    "### Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20438a17",
   "metadata": {},
   "source": [
    "1. Use Evaluation Metrics Suitable for Imbalanced Data:\n",
    "Accuracy alone is not a reliable metric for imbalanced datasets. Instead, focus on evaluation metrics that provide a more comprehensive understanding of the model's performance. Metrics such as precision, recall, F1-score, area under the ROC curve (AUC-ROC), and area under the precision-recall curve (AUC-PR) are commonly used.\n",
    "\n",
    "2. Confusion Matrix Analysis:\n",
    "Analyze the confusion matrix to gain insights into the model's performance. Pay close attention to true positive (TP), true negative (TN), false positive (FP), and false negative (FN) values. These values can help you understand the model's ability to correctly identify positive instances (sensitivity) and negative instances (specificity).\n",
    "\n",
    "3. Precision and Recall Tradeoff:\n",
    "Precision and recall are inversely related to each other. Adjusting the model's decision threshold can impact the tradeoff between precision (positive predictive value) and recall (sensitivity). Depending on the specific project requirements, you can determine the desired balance and optimize the threshold accordingly.\n",
    "\n",
    "4. Stratified Cross-Validation:\n",
    "Implement stratified cross-validation to ensure that each fold of the cross-validation maintains the same class distribution as the original dataset. This helps in obtaining more reliable and representative performance estimates.\n",
    "\n",
    "5. Resampling Techniques:\n",
    "Use resampling techniques such as oversampling the minority class or undersampling the majority class to balance the class distribution. This can help prevent the model from being biased towards the majority class and improve its performance on the minority class. However, be cautious when using resampling techniques as they can introduce some biases or risks.\n",
    "\n",
    "6. Ensemble Methods:\n",
    "Employ ensemble methods such as bagging, boosting, or stacking to leverage the power of multiple models. Ensemble methods can help mitigate the impact of imbalanced data by combining predictions from multiple models, reducing the bias towards the majority class, and improving performance on the minority class.\n",
    "\n",
    "7. Cost-Sensitive Learning:\n",
    "Incorporate the costs or misclassification penalties associated with each class into the learning process. Assign higher costs to misclassifications of the minority class to make the model more sensitive to it. This can guide the model to prioritize the correct classification of the minority class, which is often of greater importance.\n",
    "\n",
    "8. Domain Expertise and External Validation:\n",
    "Consult domain experts to gain insights into the significance and impact of correct predictions and misclassifications. Additionally, consider external validation by assessing the model's performance on independent, real-world data to validate its effectiveness in practical scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbad93f6",
   "metadata": {},
   "source": [
    "### Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460702c8",
   "metadata": {},
   "source": [
    "1. Random Under-sampling:\n",
    "Randomly select a subset of instances from the majority class to match the number of instances in the minority class. This approach helps balance the class distribution by reducing the dominance of the majority class.\n",
    "\n",
    "2. Cluster Centroids:\n",
    "Use clustering algorithms to identify clusters within the majority class and then randomly select cluster centroids (representative instances) to down-sample the majority class. This technique helps preserve the structure of the majority class while reducing its size.\n",
    "\n",
    "3. Tomek Links:\n",
    "Identify pairs of instances from different classes that are the nearest neighbors of each other. Remove the majority class instance from each pair to eliminate instances that create ambiguities near the decision boundary. This method helps improve the separation between classes.\n",
    "\n",
    "4. NearMiss:\n",
    "Select instances from the majority class based on their proximity to the minority class instances. NearMiss algorithms identify the nearest neighbors of the minority class and select instances from the majority class that are closest to these neighbors. This approach aims to retain informative majority class instances while reducing their dominance.\n",
    "\n",
    "5. Edited Nearest Neighbors (ENN):\n",
    "Examine the classification consistency of each majority class instance with its k nearest neighbors. Remove instances that are misclassified to reduce the dominance of the majority class. This technique focuses on removing potentially noisy instances.\n",
    "\n",
    "6. Combination of Under-sampling and Over-sampling:\n",
    "Combine random under-sampling of the majority class with over-sampling techniques applied to the minority class. This approach aims to balance the class distribution by reducing the majority class and increasing the minority class simultaneously. Techniques like Synthetic Minority Over-sampling Technique (SMOTE) can be used for over-sampling the minority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995da367",
   "metadata": {},
   "source": [
    "### Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b426dd4",
   "metadata": {},
   "source": [
    "Random Over-sampling:\n",
    "\n",
    "Randomly duplicate instances from the minority class to increase its size and match the number of instances in the majority class. This approach helps balance the class distribution by increasing the representation of the minority class.\n",
    "SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "\n",
    "SMOTE generates synthetic samples for the minority class by interpolating feature values between existing minority class instances. This method helps increase the number of minority class instances while preserving the underlying characteristics of the minority class.\n",
    "ADASYN (Adaptive Synthetic Sampling):\n",
    "\n",
    "ADASYN is an extension of SMOTE that generates synthetic samples for the minority class based on the density distribution of the dataset. It focuses on generating more synthetic samples for instances that are harder to learn, making it particularly useful for imbalanced datasets.\n",
    "Synthetic Minority Over-sampling TEchnique for Nominal and Continuous (SMOTE-NC):\n",
    "\n",
    "SMOTE-NC is an extension of SMOTE that handles datasets with both nominal and continuous features. It generates synthetic samples by considering both feature values and categorical information.\n",
    "Borderline-SMOTE:\n",
    "\n",
    "Borderline-SMOTE focuses on generating synthetic samples near the decision boundary between the minority and majority classes. It identifies borderline instances that are incorrectly classified or close to being misclassified and generates synthetic samples in their vicinity.\n",
    "Combination of Over-sampling and Under-sampling:\n",
    "\n",
    "Combine random over-sampling of the minority class with under-sampling techniques applied to the majority class. This approach aims to balance the class distribution by increasing the minority class and reducing the dominance of the majority class simultaneously. Techniques like random under-sampling or Tomek Links can be used for under-sampling the majority class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
