{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec580dfd",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63696aa",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique that uses the concept of gradient boosting to create a strong predictive model for regression tasks. It is an extension of the gradient boosting framework, originally developed for classification problems, and adapted to handle regression problems where the goal is to predict continuous numerical values rather than discrete class labels.\n",
    "\n",
    "In Gradient Boosting Regression, the ensemble model is built by sequentially adding weak learners (often decision trees) to the model and adjusting their parameters based on the gradients (residual errors) of the loss function with respect to the training data. The process involves the following steps:\n",
    "\n",
    "1. **Data and Initialization**: The training data is prepared with input features (X) and corresponding continuous target values (y). The initial predictions (ŷ) are often set to the average target value (or any other suitable constant) to establish a baseline.\n",
    "\n",
    "2. **Loss Function**: A loss function is defined to measure the difference between the predicted values (ŷ) and the actual target values (y). For regression tasks, the commonly used loss functions include Mean Squared Error (MSE) and Mean Absolute Error (MAE).\n",
    "\n",
    "3. **First Weak Learner**: The first weak learner (usually a decision tree with limited depth) is trained on the training data to minimize the loss function. This initial weak learner captures the most prominent patterns in the data.\n",
    "\n",
    "4. **Residuals Calculation**: The residuals (error) between the actual target values (y) and the predictions from the first weak learner (ŷ) are computed. The subsequent weak learners will be trained to predict these residuals, focusing on the remaining patterns and errors.\n",
    "\n",
    "5. **Sequential Training**: The boosting process continues with the addition of more weak learners. Each new weak learner is trained to predict the negative gradient (residuals) of the loss function with respect to the current predictions.\n",
    "\n",
    "6. **Ensemble Prediction**: The final prediction from the ensemble model is obtained by summing the predictions of all weak learners. The process of adding weak learners continues until a predefined number of iterations (n_estimators) or until the model's performance on a validation set reaches a satisfactory level.\n",
    "\n",
    "7. **Regularization**: To prevent overfitting, regularization techniques such as shrinkage (learning rate) and subsampling can be employed to limit the contribution of individual weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358cb305",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e787b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 17.62000956211771\n",
      "R-squared: -7.810004781058854\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize the predictions with the average of the target values\n",
    "        self.prediction = np.mean(y) * np.ones(len(y))\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate the negative gradient (residuals)\n",
    "            residuals = y - self.prediction\n",
    "            \n",
    "            # Train a decision tree on the negative gradient\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Update the predictions using the new tree\n",
    "            self.prediction += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "            # Store the trained tree in the list\n",
    "            self.trees.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Make predictions by summing the predictions of all trees\n",
    "        y_pred = np.sum(tree.predict(X) for tree in self.trees)\n",
    "        y_pred = y_pred/len(self.trees)\n",
    "        return y_pred\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "    return 1 - (ss_residual / ss_total)\n",
    "\n",
    "# Sample data for regression\n",
    "X_train = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y_train = np.array([2, 3, 4, 5, 6])\n",
    "\n",
    "# Create and train the gradient boosting regressor\n",
    "regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_pred_train = regressor.predict(X_train)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_train, y_pred_train)\n",
    "r2 = r_squared(y_train, y_pred_train)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af512c47",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8100c546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search - Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50}\n",
      "Grid Search - Best Mean Squared Error (MSE): 152.4032578202841\n",
      "Random Search - Best Hyperparameters: {'n_estimators': 100, 'max_depth': 2, 'learning_rate': 0.1}\n",
      "Random Search - Best Mean Squared Error (MSE): 154.13912725575588\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class GradientBoostingRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.prediction = np.mean(y) * np.ones(len(y))\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - self.prediction\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.prediction += self.learning_rate * tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = np.sum(tree.predict(X) for tree in self.trees)\n",
    "        return y_pred\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'max_depth': self.max_depth\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Sample data for regression\n",
    "X_train = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y_train = np.array([2, 3, 4, 5, 6])\n",
    "\n",
    "# Hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Create the Gradient Boosting Regressor\n",
    "regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Grid Search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(regressor, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params_grid = grid_search.best_params_\n",
    "best_mse_grid = -grid_search.best_score_\n",
    "\n",
    "print(\"Grid Search - Best Hyperparameters:\", best_params_grid)\n",
    "print(\"Grid Search - Best Mean Squared Error (MSE):\", best_mse_grid)\n",
    "\n",
    "# Random Search to find the best hyperparameters\n",
    "random_search = RandomizedSearchCV(regressor, param_distributions=param_grid, n_iter=10, cv=3, scoring='neg_mean_squared_error')\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_params_random = random_search.best_params_\n",
    "best_mse_random = -random_search.best_score_\n",
    "\n",
    "print(\"Random Search - Best Hyperparameters:\", best_params_random)\n",
    "print(\"Random Search - Best Mean Squared Error (MSE):\", best_mse_random)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8432499c",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f3ab96",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a weak learner is a simple and relatively low-complexity model that performs slightly better than random guessing on a given learning task. These weak learners are also commonly referred to as base learners.\n",
    "\n",
    "A weak learner is typically a model that is not strong enough on its own to provide accurate predictions for the entire dataset. However, when combined with other weak learners in an ensemble, they contribute to building a much more powerful predictive model. Gradient Boosting sequentially adds weak learners to the ensemble, with each subsequent learner focusing on the mistakes made by the previous ones.\n",
    "\n",
    "In the case of Gradient Boosting Regression, the weak learners are often decision trees with shallow depths (few nodes or levels) or limited number of leaf nodes. These decision trees can capture simple patterns and relationships in the data but are prone to overfitting when they become too complex.\n",
    "\n",
    "For Gradient Boosting Classification, weak learners can be shallow decision trees as well, or even simple models like stump classifiers, which are decision trees with just one split (a single decision node and two leaf nodes).\n",
    "\n",
    "The key idea behind using weak learners in Gradient Boosting is that their combination allows the ensemble model to learn complex patterns and relationships in the data effectively. By iteratively improving the model's performance with each weak learner addition, Gradient Boosting creates a strong learner that can generalize well on a variety of datasets and perform exceptionally well on the given learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9e0e1",
   "metadata": {},
   "source": [
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c3a4c",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm lies in the idea of building a strong and accurate predictive model by combining the predictions of multiple weak learners in an additive manner. The algorithm sequentially improves upon the mistakes made by the previous weak learners, focusing on the samples that are difficult to predict correctly.\n",
    "\n",
    "Here's the intuition behind the Gradient Boosting algorithm:\n",
    "\n",
    "1. **Additive Model**: Gradient Boosting builds an additive model where each new weak learner is trained to correct the errors made by the ensemble of the previous weak learners. It starts with a simple model, like the average of the target values for regression or a constant prediction for classification, and iteratively improves the model.\n",
    "\n",
    "2. **Residual Learning**: At each iteration, the algorithm identifies the remaining errors (residuals) between the current predictions and the true target values. The subsequent weak learner is then trained to predict these residuals rather than the original target values. This \"residual learning\" is crucial in boosting the performance as it helps the model to focus on the samples where the previous learners struggled.\n",
    "\n",
    "3. **Weighted Voting**: Each weak learner is given a weight based on its performance during training. The weights of the weak learners determine their influence on the final prediction. Better-performing weak learners are assigned higher weights, and they contribute more to the ensemble's output.\n",
    "\n",
    "4. **Ensemble of Weak Learners**: The final prediction of the ensemble model is the weighted sum of the predictions made by each weak learner. The combination of weak learners helps to capture different aspects and patterns of the data, leading to a more accurate and robust prediction.\n",
    "\n",
    "5. **Regularization**: To avoid overfitting and control the complexity of the ensemble, the algorithm usually employs regularization techniques. Common regularization methods include using shallow trees (limited depth), subsampling the data, and using a learning rate that scales the contribution of each weak learner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803d25a0",
   "metadata": {},
   "source": [
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2171d0",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. It starts with a simple model and then iteratively adds weak learners to improve the model's performance. The process can be summarized in the following steps:\n",
    "\n",
    "1. **Initialization**: The algorithm begins by initializing the ensemble with a simple model, often with constant predictions (e.g., the mean of target values for regression or the most frequent class for classification).\n",
    "\n",
    "2. **Compute Residuals**: At each iteration, the algorithm calculates the negative gradient (residuals) of the loss function with respect to the current predictions. The residuals represent the errors made by the current ensemble on the training data.\n",
    "\n",
    "3. **Train Weak Learner**: The next step is to train a new weak learner (e.g., a decision tree with limited depth) on the residuals calculated in the previous step. The weak learner is trained to predict these residuals, focusing on the samples where the current ensemble performs poorly.\n",
    "\n",
    "4. **Update Predictions**: After training the weak learner, its predictions are scaled by a learning rate (usually less than 1) and added to the current predictions of the ensemble. This update reduces the impact of the new weak learner to avoid overfitting and allows for a more stable learning process.\n",
    "\n",
    "5. **Update Weights**: The contribution of each weak learner to the ensemble's final prediction is determined by its performance on the training data. Better-performing weak learners are given higher weights, and weaker learners receive lower weights.\n",
    "\n",
    "6. **Repeat**: Steps 2 to 5 are repeated for a predefined number of iterations (n_estimators) or until the model's performance on a validation set converges or reaches a satisfactory level.\n",
    "\n",
    "7. **Ensemble Prediction**: The final prediction of the ensemble model is the weighted sum of the predictions made by each weak learner. The weighted combination of weak learners allows the ensemble to capture different patterns and relationships in the data, leading to a more accurate and robust prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe085a1d",
   "metadata": {},
   "source": [
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b469b553",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves understanding the underlying principles and mathematical formulations behind its sequential training process. Below are the steps involved in building the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "1. **Loss Function**: The first step is to define a loss function that quantifies the difference between the actual target values and the predicted values. For regression tasks, the commonly used loss functions include Mean Squared Error (MSE) or Mean Absolute Error (MAE), while for classification tasks, the cross-entropy loss or the deviance loss (logistic loss for binary classification) is often used.\n",
    "\n",
    "2. **Initialization**: The ensemble starts with an initial prediction, often a simple estimate like the mean of the target values for regression or the log-odds for binary classification.\n",
    "\n",
    "3. **Residual Calculation**: At each iteration, the algorithm computes the negative gradient (residuals) of the loss function with respect to the current predictions. The residuals represent the errors made by the current ensemble on the training data.\n",
    "\n",
    "4. **Weak Learner Training**: The next step is to train a new weak learner (e.g., a decision tree with limited depth) on the negative gradients calculated in the previous step. The weak learner is trained to predict the residuals, aiming to correct the mistakes made by the current ensemble.\n",
    "\n",
    "5. **Learning Rate and Weighted Update**: To control the contribution of each weak learner and avoid overfitting, the predictions of the weak learner are scaled by a learning rate (usually less than 1) and added to the current predictions of the ensemble. The learning rate acts as a regularization parameter, slowing down the learning process and making the algorithm less sensitive to the new learner's predictions.\n",
    "\n",
    "6. **Update Ensemble Prediction**: After adding the new weak learner's prediction to the ensemble, the updated ensemble prediction is used for the next iteration. The process of adding weak learners and updating the ensemble prediction continues for a predefined number of iterations (n_estimators).\n",
    "\n",
    "7. **Weighting Weak Learners**: The contribution of each weak learner to the ensemble's final prediction is determined based on its performance on the training data. Better-performing weak learners are given higher weights, and weaker learners receive lower weights. The weights are usually calculated using optimization techniques like gradient descent.\n",
    "\n",
    "8. **Ensemble Prediction**: The final prediction of the ensemble model is obtained by summing the predictions of all weak learners, each weighted by its corresponding weight. The ensemble prediction represents the combined output of all weak learners, capturing complex patterns and relationships in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
