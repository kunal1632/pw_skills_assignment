{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7f378b3",
   "metadata": {},
   "source": [
    "### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f45150f",
   "metadata": {},
   "source": [
    "A: Employee uses the company's health insurance plan.\n",
    "B: Employee is a smoker.\n",
    "\n",
    "We are given the following probabilities:\n",
    "\n",
    "- \\( P(A) \\) = Probability that an employee uses the health insurance plan = 0.70 (70%)\n",
    "- \\( P(B|A) \\) = Probability that an employee is a smoker given that they use the health insurance plan = 0.40 (40%)\n",
    "\n",
    "We want to find \\( P(B|A) \\), which represents the probability of an employee being a smoker given that they use the health insurance plan.\n",
    "\n",
    "Using Bayes' theorem:\n",
    "\n",
    "\\[$ P(B|A) = \\frac{P(A|B) \\cdot P(B)}{P(A)} \\$]\n",
    "\n",
    "Now, we need to find \\( P(A|B) \\) and \\( P(B) \\).\n",
    "\n",
    "Since we don't have the direct value of \\( P(A|B) \\), we can use the complement rule:\n",
    "\n",
    "\\[$ P(A|B) = 1 - P(\\text{not A|B}) \\$]\n",
    "\n",
    "The event \"not A\" represents an employee not using the health insurance plan.\n",
    "\n",
    "\\[$ P(\\text{not A|B}) = 1 - P(A|B) = 1 - 0.40 = 0.60 \\$]\n",
    "\n",
    "Next, we find \\( P(B) \\):\n",
    "\n",
    "\\[$ P(B) = P(B|A) \\cdot P(A) + P(B|\\text{not A}) \\cdot P(\\text{not A}) \\$]\n",
    "\n",
    "We are not given the probability of an employee being a smoker given that they do not use the health insurance plan (\\($ P(B|\\text{not A}) \\$)), but we can infer it using the complement rule:\n",
    "\n",
    "\\[$ P(B|\\text{not A}) = 1 - P(\\text{not B|\\text{not A}}) \\$]\n",
    "\n",
    "The event \"not B\" represents an employee not being a smoker.\n",
    "\n",
    "Assuming that all non-users of the health insurance plan are non-smokers:\n",
    "\n",
    "\\[$ P(B|\\text{not A}) = 1 - 0 = 1 \\$]\n",
    "\n",
    "Since all non-users are non-smokers, the probability of a smoker not using the health insurance plan is 0.\n",
    "\n",
    "Now we can calculate \\( P(B) \\):\n",
    "\n",
    "\\[$ P(B) = P(B|A) \\cdot P(A) + P(B|\\text{not A}) \\cdot P(\\text{not A}) \\$]\n",
    "\n",
    "\\[$ P(B) = 0.40 \\cdot 0.70 + 1 \\cdot (1 - 0.70) \\$]\n",
    "\n",
    "\\[$ P(B) = 0.28 + 0.30 = 0.58 \\$]\n",
    "\n",
    "Now we can use Bayes' theorem to find \\( P(B|A) \\):\n",
    "\n",
    "\\[$ P(B|A) = \\frac{P(A|B) \\cdot P(B)}{P(A)} \\$]\n",
    "\n",
    "\\[$ P(B|A) = \\frac{0.40 \\cdot 0.58}{0.70} \\$]\n",
    "\n",
    "\\[$ P(B|A) = \\frac{0.232}{0.70} \\approx 0.3314 \\$]\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.3314 or 33.14%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e27106",
   "metadata": {},
   "source": [
    "### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a878ff",
   "metadata": {},
   "source": [
    "\n",
    "The difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the types of data they are designed to handle and the way they model the features.\n",
    "\n",
    "#### Bernoulli Naive Bayes:\n",
    "\n",
    "- Suitable for binary or boolean features, where each feature can take only two possible values (e.g., 0 or 1, yes or no).\n",
    "- Assumes that each feature is binary and independent of each other given the class label.\n",
    "- It models the presence or absence of each feature in a document or instance.\n",
    "- Often used in text classification tasks, where the presence or absence of specific words or features in a document is relevant for classification.\n",
    "#### Multinomial Naive Bayes:\n",
    "\n",
    "- Suitable for discrete features that represent counts or frequencies of occurrences (e.g., word counts in a document).\n",
    "- Assumes that each feature follows a multinomial distribution, meaning it counts the occurrences of each feature for each class.\n",
    "- It models the frequency of each feature in a document or instance, considering the number of occurrences.\n",
    "Commonly used in text classification tasks, such as spam detection, sentiment analysis, and document categorization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515c394",
   "metadata": {},
   "source": [
    "### Q3. How does Bernoulli Naive Bayes handle missing values?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cff20f",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes handles missing values in a straightforward manner by simply ignoring the missing values during the calculation of probabilities. Since Bernoulli Naive Bayes is designed to work with binary features, where each feature can only take two possible values (e.g., 0 or 1, yes or no), any missing value is treated as a separate category or state.\n",
    "\n",
    "Here's how missing values are handled in Bernoulli Naive Bayes:\n",
    "\n",
    "1. **Training Phase**:\n",
    "   - During the training phase, the algorithm calculates the probabilities of each feature (0 or 1) occurring for each class based on the available training data. The presence of a feature is denoted as 1, and its absence is denoted as 0.\n",
    "   - If a specific feature value is missing for a data point in the training data, it is simply ignored during the probability estimation. The algorithm continues to calculate the probabilities based on the non-missing feature values for that data point.\n",
    "\n",
    "2. **Prediction Phase**:\n",
    "   - When making predictions for new instances in the prediction phase, if a feature value is missing for a particular instance, the algorithm will again ignore that missing value during the probability calculation for each class.\n",
    "   - The missing value is treated as if it were a separate feature state and is not considered in the probability estimation for that particular feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2268834",
   "metadata": {},
   "source": [
    "### Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d6aaf6",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that assumes continuous features, specifically, features that follow a Gaussian (normal) distribution within each class. While it is often used for binary or two-class classification problems, it can also be extended to handle multi-class classification problems by employing the \"one-vs-all\" or \"one-vs-one\" strategies.\n",
    "\n",
    "Here's how Gaussian Naive Bayes can be used for multi-class classification:\n",
    "\n",
    "1. **One-vs-All (OvA) Strategy**:\n",
    "   - In the OvA strategy, the multi-class classification problem is divided into multiple binary classification subproblems.\n",
    "   - For each class, a binary classifier is trained to distinguish that class from all other classes (i.e., one class against the rest).\n",
    "   - During the prediction phase, each classifier's probability outputs are collected, and the class with the highest probability is selected as the predicted class.\n",
    "\n",
    "2. **One-vs-One (OvO) Strategy**:\n",
    "   - In the OvO strategy, the multi-class classification problem is divided into pairwise binary classification subproblems.\n",
    "   - For each pair of classes, a binary classifier is trained to distinguish between those two classes only.\n",
    "   - During the prediction phase, each classifier's probability outputs are collected, and a voting scheme is used to determine the predicted class based on the most frequently selected class in the binary classifications.\n",
    "\n",
    "Both strategies allow Gaussian Naive Bayes to handle multi-class classification problems effectively. The choice between the OvA and OvO strategies depends on factors such as the dataset size, computational resources, and the classifier's performance on the specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1934b84d",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "\n",
    "#### Data preparation:\n",
    "\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "#### Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default hyperparameters for each classifier.\n",
    "#### Results:\n",
    "- Report the following performance metrics for each classifier:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "#### Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "Note: This dataset contains a binary classification problem with multiple features. The dataset is\n",
    "relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
    "Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5096e867",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Bernoulli Naive Bayes Classifier:\n",
      "Accuracy: 0.88\n",
      "Precision: 0.89\n",
      "Recall: 0.82\n",
      "F1 Score: 0.85\n",
      "\n",
      "\n",
      "Results for Multinomial Naive Bayes Classifier:\n",
      "Accuracy: 0.79\n",
      "Precision: 0.74\n",
      "Recall: 0.72\n",
      "F1 Score: 0.73\n",
      "\n",
      "\n",
      "Results for Gaussian Naive Bayes Classifier:\n",
      "Accuracy: 0.82\n",
      "Precision: 0.71\n",
      "Recall: 0.96\n",
      "F1 Score: 0.81\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "# Step 2: Load and preprocess the data (assuming you have downloaded the data as \"spambase.csv\")\n",
    "data = pd.read_csv(\"spambase.data\",header=None)\n",
    "X = data.iloc[:,:57]\n",
    "y = data.iloc[:,57]\n",
    "\n",
    "# Step 3: Implement and evaluate the classifiers\n",
    "def evaluate_classifier(classifier, name):\n",
    "    accuracy = np.mean(cross_val_score(classifier, X, y, cv=10, scoring=\"accuracy\"))\n",
    "    precision = np.mean(cross_val_score(classifier, X, y, cv=10, scoring=\"precision\"))\n",
    "    recall = np.mean(cross_val_score(classifier, X, y, cv=10, scoring=\"recall\"))\n",
    "    f1_score = np.mean(cross_val_score(classifier, X, y, cv=10, scoring=\"f1\"))\n",
    "    \n",
    "    print(f\"Results for {name} Naive Bayes Classifier:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1_score:.2f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Instantiate and evaluate the classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "evaluate_classifier(bernoulli_nb, \"Bernoulli\")\n",
    "evaluate_classifier(multinomial_nb, \"Multinomial\")\n",
    "evaluate_classifier(gaussian_nb, \"Gaussian\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ba3080",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "\n",
    "- The results obtained for each classifier show that Bernoulli naive bayes performed the best in this specific dataset with the highest accuracy of 88%.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
